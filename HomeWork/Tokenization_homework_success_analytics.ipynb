{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07b40f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "added_text = \"\"\"Lexical tokenization is conversion of a text into (semantically or syntactically) meaningful \n",
    "lexical tokens belonging to categories defined by a \"lexer\" program. \n",
    "In case of a natural language, those categories include nouns, verbs, \n",
    "adjectives, punctuations etc. In case of a programming language, \n",
    "the categories include identifiers, operators, grouping symbols and data types. \n",
    "Lexical tokenization is not the same process as the probabilistic tokenization, \n",
    "used for large language model's 3689 data preprocessing, that encode text into numerical tokens, using byte pair encoding.The first stage, the scanner, \n",
    "is usually based on a finite-state machine (FSM). It has encoded within it information on \n",
    "the possible sequences of characters that can be contained within any of the tokens it handles \n",
    "(individual instances of these character sequences are termed lexemes). For example, \n",
    "an integer lexeme may contain !@%$ any sequence of numerical digit characters. \n",
    "In many cases, the first non-whitespace character can be used to deduce \n",
    "the kind of token that follows and subsequent input characters are then \n",
    "processed one at a time until reaching a character that is not in the \n",
    "set of characters acceptable for that token (this is termed the maximal munch, \n",
    "or longest match, rule). In some languages, 1492 the lexeme creation rules \n",
    "are more complex and may involve backtracking over previously read characters. \n",
    "For example, in C, one 'L' character is not enough to distinguish between an identifier \n",
    "that begins with 'L' and a wide-character string literal.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8522d0a1",
   "metadata": {},
   "source": [
    "# Adding lines in text document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23ab8027",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"homework.txt\",\"w\") as file:\n",
    "    file.write(added_text)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35af683f",
   "metadata": {},
   "source": [
    "# Reading document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89dedcc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"homework.txt\",\"r\") as file:\n",
    "    file_content = file.read()\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e94d6775",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Lexical tokenization is conversion of a text into (semantically or syntactically) meaningful \\nlexical tokens belonging to categories defined by a \"lexer\" program. \\nIn case of a natural language, those categories include nouns, verbs, \\nadjectives, punctuations etc. In case of a programming language, \\nthe categories include identifiers, operators, grouping symbols and data types. \\nLexical tokenization is not the same process as the probabilistic tokenization, \\nused for large language model\\'s 3689 data preprocessing, that encode text into numerical tokens, using byte pair encoding.The first stage, the scanner, \\nis usually based on a finite-state machine (FSM). It has encoded within it information on \\nthe possible sequences of characters that can be contained within any of the tokens it handles \\n(individual instances of these character sequences are termed lexemes). For example, \\nan integer lexeme may contain !@%$ any sequence of numerical digit characters. \\nIn many cases, the first non-whitespace character can be used to deduce \\nthe kind of token that follows and subsequent input characters are then \\nprocessed one at a time until reaching a character that is not in the \\nset of characters acceptable for that token (this is termed the maximal munch, \\nor longest match, rule). In some languages, 1492 the lexeme creation rules \\nare more complex and may involve backtracking over previously read characters. \\nFor example, in C, one \\'L\\' character is not enough to distinguish between an identifier \\nthat begins with \\'L\\' and a wide-character string literal.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d0c9137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\sumit kumar sahoo\\anaconda3\\lib\\site-packages (3.6.1)\n",
      "Collecting nltk\n",
      "  Using cached nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\sumit kumar sahoo\\anaconda3\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Collecting regex>=2021.8.3\n",
      "  Downloading regex-2023.8.8-cp38-cp38-win_amd64.whl (268 kB)\n",
      "Requirement already satisfied: joblib in c:\\users\\sumit kumar sahoo\\anaconda3\\lib\\site-packages (from nltk) (1.0.1)\n",
      "Requirement already satisfied: click in c:\\users\\sumit kumar sahoo\\anaconda3\\lib\\site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\sumit kumar sahoo\\anaconda3\\lib\\site-packages (from tqdm->nltk) (0.4.4)\n",
      "Installing collected packages: regex, nltk\n",
      "Successfully installed nltk-3.8.1 regex-2023.8.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script nltk.exe is installed in 'C:\\Users\\Sumit Kumar Sahoo\\AppData\\Roaming\\Python\\Python38\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    }
   ],
   "source": [
    "!pip install --user -U nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e5f362ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\sumit kumar sahoo\\anaconda3\\lib\\site-packages (3.2.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\sumit kumar sahoo\\anaconda3\\lib\\site-packages (from spacy) (1.0.6)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\sumit kumar sahoo\\anaconda3\\lib\\site-packages (from spacy) (2.0.6)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\sumit kumar sahoo\\anaconda3\\lib\\site-packages (from spacy) (1.0.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in c:\\users\\sumit kumar sahoo\\anaconda3\\lib\\site-packages (from spacy) (3.0.8)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\sumit kumar sahoo\\anaconda3\\lib\\site-packages (from spacy) (0.7.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\sumit kumar sahoo\\anaconda3\\lib\\site-packages (from spacy) (3.0.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\sumit kumar sahoo\\anaconda3\\lib\\site-packages (from spacy) (21.3)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\sumit kumar sahoo\\anaconda3\\lib\\site-packages (from spacy) (3.0.6)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\sumit kumar sahoo\\anaconda3\\lib\\site-packages (from spacy) (4.65.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\sumit kumar sahoo\\anaconda3\\lib\\site-packages (from spacy) (52.0.0.post20210125)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in c:\\users\\sumit kumar sahoo\\anaconda3\\lib\\site-packages (from spacy) (8.0.13)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\sumit kumar sahoo\\anaconda3\\lib\\site-packages (from spacy) (0.6.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in c:\\users\\sumit kumar sahoo\\anaconda3\\lib\\site-packages (from spacy) (2.4.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\sumit kumar sahoo\\anaconda3\\lib\\site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in c:\\users\\sumit kumar sahoo\\anaconda3\\lib\\site-packages (from spacy) (0.4.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in c:\\users\\sumit kumar sahoo\\anaconda3\\lib\\site-packages (from spacy) (0.8.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\sumit kumar sahoo\\anaconda3\\lib\\site-packages (from spacy) (2.25.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in c:\\users\\sumit kumar sahoo\\anaconda3\\lib\\site-packages (from spacy) (1.8.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\sumit kumar sahoo\\anaconda3\\lib\\site-packages (from spacy) (2.0.6)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\sumit kumar sahoo\\anaconda3\\lib\\site-packages (from spacy) (1.22.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\sumit kumar sahoo\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy) (2.4.7)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in c:\\users\\sumit kumar sahoo\\anaconda3\\lib\\site-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\sumit kumar sahoo\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy) (4.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sumit kumar sahoo\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.12.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\sumit kumar sahoo\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\sumit kumar sahoo\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\sumit kumar sahoo\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (4.0.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\sumit kumar sahoo\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\sumit kumar sahoo\\anaconda3\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\sumit kumar sahoo\\anaconda3\\lib\\site-packages (from jinja2->spacy) (2.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6d736909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-md==3.2.0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-31 09:00:58.049749: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\n",
      "2023-08-31 09:00:58.049943: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.2.0/en_core_web_md-3.2.0-py3-none-any.whl (45.7 MB)\n",
      "Requirement already satisfied: spacy<3.3.0,>=3.2.0 in c:\\users\\sumit kumar sahoo\\anaconda3\\lib\\site-packages (from en-core-web-md==3.2.0) (3.2.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\sumit kumar sahoo\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (3.0.6)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\sumit kumar sahoo\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (0.7.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\sumit kumar sahoo\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (1.0.6)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in c:\\users\\sumit kumar sahoo\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (0.8.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\sumit kumar sahoo\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (2.0.6)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in c:\\users\\sumit kumar sahoo\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (3.0.8)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\sumit kumar sahoo\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (4.65.0)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in c:\\users\\sumit kumar sahoo\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (0.4.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\sumit kumar sahoo\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (3.3.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\sumit kumar sahoo\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (3.0.3)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\sumit kumar sahoo\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (0.6.1)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in c:\\users\\sumit kumar sahoo\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (8.0.13)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\sumit kumar sahoo\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (2.25.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\sumit kumar sahoo\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (2.0.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\sumit kumar sahoo\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (52.0.0.post20210125)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\sumit kumar sahoo\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (1.0.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\sumit kumar sahoo\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (21.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in c:\\users\\sumit kumar sahoo\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (1.8.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in c:\\users\\sumit kumar sahoo\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (2.4.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\sumit kumar sahoo\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (1.22.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\sumit kumar sahoo\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (2.4.7)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in c:\\users\\sumit kumar sahoo\\anaconda3\\lib\\site-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\sumit kumar sahoo\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (4.4.0)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\sumit kumar sahoo\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\sumit kumar sahoo\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\sumit kumar sahoo\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (1.26.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sumit kumar sahoo\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (2020.12.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\sumit kumar sahoo\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (0.4.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\sumit kumar sahoo\\anaconda3\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\sumit kumar sahoo\\anaconda3\\lib\\site-packages (from jinja2->spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (2.0.1)\n",
      "Installing collected packages: en-core-web-md\n",
      "Successfully installed en-core-web-md-3.2.0\n",
      "[+] Download and installation successful\n",
      "You can now load the package via spacy.load('en_core_web_md')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_md "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "63e04416",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sentence tokenize and word tokenize using NLTK library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c4ba4eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Lexical tokenization is conversion of a text into (semantically or syntactically) meaningful \\nlexical tokens belonging to categories defined by a \"lexer\" program.',\n",
       " 'In case of a natural language, those categories include nouns, verbs, \\nadjectives, punctuations etc.',\n",
       " 'In case of a programming language, \\nthe categories include identifiers, operators, grouping symbols and data types.',\n",
       " \"Lexical tokenization is not the same process as the probabilistic tokenization, \\nused for large language model's 3689 data preprocessing, that encode text into numerical tokens, using byte pair encoding.The first stage, the scanner, \\nis usually based on a finite-state machine (FSM).\",\n",
       " 'It has encoded within it information on \\nthe possible sequences of characters that can be contained within any of the tokens it handles \\n(individual instances of these character sequences are termed lexemes).',\n",
       " 'For example, \\nan integer lexeme may contain !',\n",
       " '@%$ any sequence of numerical digit characters.',\n",
       " 'In many cases, the first non-whitespace character can be used to deduce \\nthe kind of token that follows and subsequent input characters are then \\nprocessed one at a time until reaching a character that is not in the \\nset of characters acceptable for that token (this is termed the maximal munch, \\nor longest match, rule).',\n",
       " 'In some languages, 1492 the lexeme creation rules \\nare more complex and may involve backtracking over previously read characters.',\n",
       " \"For example, in C, one 'L' character is not enough to distinguish between an identifier \\nthat begins with 'L' and a wide-character string literal.\"]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "sent_tokenize(file_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3def612",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Lexical',\n",
       " 'tokenization',\n",
       " 'is',\n",
       " 'conversion',\n",
       " 'of',\n",
       " 'a',\n",
       " 'text',\n",
       " 'into',\n",
       " '(',\n",
       " 'semantically',\n",
       " 'or',\n",
       " 'syntactically',\n",
       " ')',\n",
       " 'meaningful',\n",
       " 'lexical',\n",
       " 'tokens',\n",
       " 'belonging',\n",
       " 'to',\n",
       " 'categories',\n",
       " 'defined',\n",
       " 'by',\n",
       " 'a',\n",
       " '``',\n",
       " 'lexer',\n",
       " \"''\",\n",
       " 'program',\n",
       " '.',\n",
       " 'In',\n",
       " 'case',\n",
       " 'of',\n",
       " 'a',\n",
       " 'natural',\n",
       " 'language',\n",
       " ',',\n",
       " 'those',\n",
       " 'categories',\n",
       " 'include',\n",
       " 'nouns',\n",
       " ',',\n",
       " 'verbs',\n",
       " ',',\n",
       " 'adjectives',\n",
       " ',',\n",
       " 'punctuations',\n",
       " 'etc',\n",
       " '.',\n",
       " 'In',\n",
       " 'case',\n",
       " 'of',\n",
       " 'a',\n",
       " 'programming',\n",
       " 'language',\n",
       " ',',\n",
       " 'the',\n",
       " 'categories',\n",
       " 'include',\n",
       " 'identifiers',\n",
       " ',',\n",
       " 'operators',\n",
       " ',',\n",
       " 'grouping',\n",
       " 'symbols',\n",
       " 'and',\n",
       " 'data',\n",
       " 'types',\n",
       " '.',\n",
       " 'Lexical',\n",
       " 'tokenization',\n",
       " 'is',\n",
       " 'not',\n",
       " 'the',\n",
       " 'same',\n",
       " 'process',\n",
       " 'as',\n",
       " 'the',\n",
       " 'probabilistic',\n",
       " 'tokenization',\n",
       " ',',\n",
       " 'used',\n",
       " 'for',\n",
       " 'large',\n",
       " 'language',\n",
       " 'model',\n",
       " \"'s\",\n",
       " '3689',\n",
       " 'data',\n",
       " 'preprocessing',\n",
       " ',',\n",
       " 'that',\n",
       " 'encode',\n",
       " 'text',\n",
       " 'into',\n",
       " 'numerical',\n",
       " 'tokens',\n",
       " ',',\n",
       " 'using',\n",
       " 'byte',\n",
       " 'pair',\n",
       " 'encoding.The',\n",
       " 'first',\n",
       " 'stage',\n",
       " ',',\n",
       " 'the',\n",
       " 'scanner',\n",
       " ',',\n",
       " 'is',\n",
       " 'usually',\n",
       " 'based',\n",
       " 'on',\n",
       " 'a',\n",
       " 'finite-state',\n",
       " 'machine',\n",
       " '(',\n",
       " 'FSM',\n",
       " ')',\n",
       " '.',\n",
       " 'It',\n",
       " 'has',\n",
       " 'encoded',\n",
       " 'within',\n",
       " 'it',\n",
       " 'information',\n",
       " 'on',\n",
       " 'the',\n",
       " 'possible',\n",
       " 'sequences',\n",
       " 'of',\n",
       " 'characters',\n",
       " 'that',\n",
       " 'can',\n",
       " 'be',\n",
       " 'contained',\n",
       " 'within',\n",
       " 'any',\n",
       " 'of',\n",
       " 'the',\n",
       " 'tokens',\n",
       " 'it',\n",
       " 'handles',\n",
       " '(',\n",
       " 'individual',\n",
       " 'instances',\n",
       " 'of',\n",
       " 'these',\n",
       " 'character',\n",
       " 'sequences',\n",
       " 'are',\n",
       " 'termed',\n",
       " 'lexemes',\n",
       " ')',\n",
       " '.',\n",
       " 'For',\n",
       " 'example',\n",
       " ',',\n",
       " 'an',\n",
       " 'integer',\n",
       " 'lexeme',\n",
       " 'may',\n",
       " 'contain',\n",
       " '!',\n",
       " '@',\n",
       " '%',\n",
       " '$',\n",
       " 'any',\n",
       " 'sequence',\n",
       " 'of',\n",
       " 'numerical',\n",
       " 'digit',\n",
       " 'characters',\n",
       " '.',\n",
       " 'In',\n",
       " 'many',\n",
       " 'cases',\n",
       " ',',\n",
       " 'the',\n",
       " 'first',\n",
       " 'non-whitespace',\n",
       " 'character',\n",
       " 'can',\n",
       " 'be',\n",
       " 'used',\n",
       " 'to',\n",
       " 'deduce',\n",
       " 'the',\n",
       " 'kind',\n",
       " 'of',\n",
       " 'token',\n",
       " 'that',\n",
       " 'follows',\n",
       " 'and',\n",
       " 'subsequent',\n",
       " 'input',\n",
       " 'characters',\n",
       " 'are',\n",
       " 'then',\n",
       " 'processed',\n",
       " 'one',\n",
       " 'at',\n",
       " 'a',\n",
       " 'time',\n",
       " 'until',\n",
       " 'reaching',\n",
       " 'a',\n",
       " 'character',\n",
       " 'that',\n",
       " 'is',\n",
       " 'not',\n",
       " 'in',\n",
       " 'the',\n",
       " 'set',\n",
       " 'of',\n",
       " 'characters',\n",
       " 'acceptable',\n",
       " 'for',\n",
       " 'that',\n",
       " 'token',\n",
       " '(',\n",
       " 'this',\n",
       " 'is',\n",
       " 'termed',\n",
       " 'the',\n",
       " 'maximal',\n",
       " 'munch',\n",
       " ',',\n",
       " 'or',\n",
       " 'longest',\n",
       " 'match',\n",
       " ',',\n",
       " 'rule',\n",
       " ')',\n",
       " '.',\n",
       " 'In',\n",
       " 'some',\n",
       " 'languages',\n",
       " ',',\n",
       " '1492',\n",
       " 'the',\n",
       " 'lexeme',\n",
       " 'creation',\n",
       " 'rules',\n",
       " 'are',\n",
       " 'more',\n",
       " 'complex',\n",
       " 'and',\n",
       " 'may',\n",
       " 'involve',\n",
       " 'backtracking',\n",
       " 'over',\n",
       " 'previously',\n",
       " 'read',\n",
       " 'characters',\n",
       " '.',\n",
       " 'For',\n",
       " 'example',\n",
       " ',',\n",
       " 'in',\n",
       " 'C',\n",
       " ',',\n",
       " 'one',\n",
       " \"'\",\n",
       " 'L',\n",
       " \"'\",\n",
       " 'character',\n",
       " 'is',\n",
       " 'not',\n",
       " 'enough',\n",
       " 'to',\n",
       " 'distinguish',\n",
       " 'between',\n",
       " 'an',\n",
       " 'identifier',\n",
       " 'that',\n",
       " 'begins',\n",
       " 'with',\n",
       " \"'\",\n",
       " 'L',\n",
       " \"'\",\n",
       " 'and',\n",
       " 'a',\n",
       " 'wide-character',\n",
       " 'string',\n",
       " 'literal',\n",
       " '.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "word_tokenize(file_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f83ef700",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Lexical',\n",
       " 'tokenization',\n",
       " 'is',\n",
       " 'conversion',\n",
       " 'of',\n",
       " 'a',\n",
       " 'text',\n",
       " 'into',\n",
       " '(',\n",
       " 'semantically',\n",
       " 'or',\n",
       " 'syntactically',\n",
       " ')',\n",
       " 'meaningful',\n",
       " 'lexical',\n",
       " 'tokens',\n",
       " 'belonging',\n",
       " 'to',\n",
       " 'categories',\n",
       " 'defined',\n",
       " 'by',\n",
       " 'a',\n",
       " '\"',\n",
       " 'lexer',\n",
       " '\"',\n",
       " 'program',\n",
       " '.',\n",
       " 'In',\n",
       " 'case',\n",
       " 'of',\n",
       " 'a',\n",
       " 'natural',\n",
       " 'language',\n",
       " ',',\n",
       " 'those',\n",
       " 'categories',\n",
       " 'include',\n",
       " 'nouns',\n",
       " ',',\n",
       " 'verbs',\n",
       " ',',\n",
       " 'adjectives',\n",
       " ',',\n",
       " 'punctuations',\n",
       " 'etc',\n",
       " '.',\n",
       " 'In',\n",
       " 'case',\n",
       " 'of',\n",
       " 'a',\n",
       " 'programming',\n",
       " 'language',\n",
       " ',',\n",
       " 'the',\n",
       " 'categories',\n",
       " 'include',\n",
       " 'identifiers',\n",
       " ',',\n",
       " 'operators',\n",
       " ',',\n",
       " 'grouping',\n",
       " 'symbols',\n",
       " 'and',\n",
       " 'data',\n",
       " 'types',\n",
       " '.',\n",
       " 'Lexical',\n",
       " 'tokenization',\n",
       " 'is',\n",
       " 'not',\n",
       " 'the',\n",
       " 'same',\n",
       " 'process',\n",
       " 'as',\n",
       " 'the',\n",
       " 'probabilistic',\n",
       " 'tokenization',\n",
       " ',',\n",
       " 'used',\n",
       " 'for',\n",
       " 'large',\n",
       " 'language',\n",
       " \"model's\",\n",
       " '3689',\n",
       " 'data',\n",
       " 'preprocessing',\n",
       " ',',\n",
       " 'that',\n",
       " 'encode',\n",
       " 'text',\n",
       " 'into',\n",
       " 'numerical',\n",
       " 'tokens',\n",
       " ',',\n",
       " 'using',\n",
       " 'byte',\n",
       " 'pair',\n",
       " 'encoding.The',\n",
       " 'first',\n",
       " 'stage',\n",
       " ',',\n",
       " 'the',\n",
       " 'scanner',\n",
       " ',',\n",
       " 'is',\n",
       " 'usually',\n",
       " 'based',\n",
       " 'on',\n",
       " 'a',\n",
       " 'finite-state',\n",
       " 'machine',\n",
       " '(',\n",
       " 'FSM',\n",
       " ')',\n",
       " '.',\n",
       " 'It',\n",
       " 'has',\n",
       " 'encoded',\n",
       " 'within',\n",
       " 'it',\n",
       " 'information',\n",
       " 'on',\n",
       " 'the',\n",
       " 'possible',\n",
       " 'sequences',\n",
       " 'of',\n",
       " 'characters',\n",
       " 'that',\n",
       " 'can',\n",
       " 'be',\n",
       " 'contained',\n",
       " 'within',\n",
       " 'any',\n",
       " 'of',\n",
       " 'the',\n",
       " 'tokens',\n",
       " 'it',\n",
       " 'handles',\n",
       " '(',\n",
       " 'individual',\n",
       " 'instances',\n",
       " 'of',\n",
       " 'these',\n",
       " 'character',\n",
       " 'sequences',\n",
       " 'are',\n",
       " 'termed',\n",
       " 'lexemes',\n",
       " ')',\n",
       " '.',\n",
       " 'For',\n",
       " 'example',\n",
       " ',',\n",
       " 'an',\n",
       " 'integer',\n",
       " 'lexeme',\n",
       " 'may',\n",
       " 'contain',\n",
       " '!',\n",
       " '@',\n",
       " '%',\n",
       " '$',\n",
       " 'any',\n",
       " 'sequence',\n",
       " 'of',\n",
       " 'numerical',\n",
       " 'digit',\n",
       " 'characters',\n",
       " '.',\n",
       " 'In',\n",
       " 'many',\n",
       " 'cases',\n",
       " ',',\n",
       " 'the',\n",
       " 'first',\n",
       " 'non-whitespace',\n",
       " 'character',\n",
       " 'can',\n",
       " 'be',\n",
       " 'used',\n",
       " 'to',\n",
       " 'deduce',\n",
       " 'the',\n",
       " 'kind',\n",
       " 'of',\n",
       " 'token',\n",
       " 'that',\n",
       " 'follows',\n",
       " 'and',\n",
       " 'subsequent',\n",
       " 'input',\n",
       " 'characters',\n",
       " 'are',\n",
       " 'then',\n",
       " 'processed',\n",
       " 'one',\n",
       " 'at',\n",
       " 'a',\n",
       " 'time',\n",
       " 'until',\n",
       " 'reaching',\n",
       " 'a',\n",
       " 'character',\n",
       " 'that',\n",
       " 'is',\n",
       " 'not',\n",
       " 'in',\n",
       " 'the',\n",
       " 'set',\n",
       " 'of',\n",
       " 'characters',\n",
       " 'acceptable',\n",
       " 'for',\n",
       " 'that',\n",
       " 'token',\n",
       " '(',\n",
       " 'this',\n",
       " 'is',\n",
       " 'termed',\n",
       " 'the',\n",
       " 'maximal',\n",
       " 'munch',\n",
       " ',',\n",
       " 'or',\n",
       " 'longest',\n",
       " 'match',\n",
       " ',',\n",
       " 'rule',\n",
       " ')',\n",
       " '.',\n",
       " 'In',\n",
       " 'some',\n",
       " 'languages',\n",
       " ',',\n",
       " '1492',\n",
       " 'the',\n",
       " 'lexeme',\n",
       " 'creation',\n",
       " 'rules',\n",
       " 'are',\n",
       " 'more',\n",
       " 'complex',\n",
       " 'and',\n",
       " 'may',\n",
       " 'involve',\n",
       " 'backtracking',\n",
       " 'over',\n",
       " 'previously',\n",
       " 'read',\n",
       " 'characters',\n",
       " '.',\n",
       " 'For',\n",
       " 'example',\n",
       " ',',\n",
       " 'in',\n",
       " 'C',\n",
       " ',',\n",
       " 'one',\n",
       " \"'\",\n",
       " 'L',\n",
       " \"'\",\n",
       " 'character',\n",
       " 'is',\n",
       " 'not',\n",
       " 'enough',\n",
       " 'to',\n",
       " 'distinguish',\n",
       " 'between',\n",
       " 'an',\n",
       " 'identifier',\n",
       " 'that',\n",
       " 'begins',\n",
       " 'with',\n",
       " \"'\",\n",
       " 'L',\n",
       " \"'\",\n",
       " 'and',\n",
       " 'a',\n",
       " 'wide-character',\n",
       " 'string',\n",
       " 'literal',\n",
       " '.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tweet_tokenizer = TweetTokenizer()\n",
    "tweet_tokenizer.tokenize(file_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8632cf48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Lexical', 'tokenization', 'is', 'conversion', 'of', 'a', 'text', 'into', '(', 'semantically', 'or', 'syntactically', ')', 'meaningful', 'lexical', 'tokens', 'belonging', 'to', 'categories', 'defined', 'by', 'a', '\"', 'lexer', '\"', 'program', '.', 'In', 'case', 'of', 'a', 'natural', 'language', ',', 'those', 'categories', 'include', 'nouns', ',', 'verbs', ',', 'adjectives', ',', 'punctuations', 'etc', '.', 'In', 'case', 'of', 'a', 'programming', 'language', ',', 'the', 'categories', 'include', 'identifiers', ',', 'operators', ',', 'grouping', 'symbols', 'and', 'data', 'types', '.', 'Lexical', 'tokenization', 'is', 'not', 'the', 'same', 'process', 'as', 'the', 'probabilistic', 'tokenization', ',', 'used', 'for', 'large', 'language', 'model', \"'\", 's', '3689', 'data', 'preprocessing', ',', 'that', 'encode', 'text', 'into', 'numerical', 'tokens', ',', 'using', 'byte', 'pair', 'encoding', '.', 'The', 'first', 'stage', ',', 'the', 'scanner', ',', 'is', 'usually', 'based', 'on', 'a', 'finite', '-', 'state', 'machine', '(', 'FSM', ').', 'It', 'has', 'encoded', 'within', 'it', 'information', 'on', 'the', 'possible', 'sequences', 'of', 'characters', 'that', 'can', 'be', 'contained', 'within', 'any', 'of', 'the', 'tokens', 'it', 'handles', '(', 'individual', 'instances', 'of', 'these', 'character', 'sequences', 'are', 'termed', 'lexemes', ').', 'For', 'example', ',', 'an', 'integer', 'lexeme', 'may', 'contain', '!@%$', 'any', 'sequence', 'of', 'numerical', 'digit', 'characters', '.', 'In', 'many', 'cases', ',', 'the', 'first', 'non', '-', 'whitespace', 'character', 'can', 'be', 'used', 'to', 'deduce', 'the', 'kind', 'of', 'token', 'that', 'follows', 'and', 'subsequent', 'input', 'characters', 'are', 'then', 'processed', 'one', 'at', 'a', 'time', 'until', 'reaching', 'a', 'character', 'that', 'is', 'not', 'in', 'the', 'set', 'of', 'characters', 'acceptable', 'for', 'that', 'token', '(', 'this', 'is', 'termed', 'the', 'maximal', 'munch', ',', 'or', 'longest', 'match', ',', 'rule', ').', 'In', 'some', 'languages', ',', '1492', 'the', 'lexeme', 'creation', 'rules', 'are', 'more', 'complex', 'and', 'may', 'involve', 'backtracking', 'over', 'previously', 'read', 'characters', '.', 'For', 'example', ',', 'in', 'C', ',', 'one', \"'\", 'L', \"'\", 'character', 'is', 'not', 'enough', 'to', 'distinguish', 'between', 'an', 'identifier', 'that', 'begins', 'with', \"'\", 'L', \"'\", 'and', 'a', 'wide', '-', 'character', 'string', 'literal', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    "\n",
    "print(wordpunct_tokenize(file_content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fbd517",
   "metadata": {},
   "source": [
    "# sentence tokenize and word tokenize using spacy library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "768a33b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lexical tokenization is conversion of a text into (semantically or syntactically) meaningful \n",
      "lexical tokens belonging to categories defined by a \"lexer\" program.\n",
      "\n",
      "In case of a natural language, those categories include nouns, verbs, \n",
      "adjectives, punctuations etc.\n",
      "In case of a programming language, \n",
      "the categories include identifiers, operators, grouping symbols and data types.\n",
      "\n",
      "Lexical tokenization is not the same process as the probabilistic tokenization, \n",
      "used for large language model's 3689 data preprocessing, that encode text into numerical tokens, using byte pair encoding.\n",
      "The first stage, the scanner, \n",
      "is usually based on a finite-state machine (FSM).\n",
      "It has encoded within it information on \n",
      "the possible sequences of characters that can be contained within any of the tokens it handles \n",
      "(individual instances of these character sequences are termed lexemes).\n",
      "For example, \n",
      "an integer lexeme may contain !\n",
      "@%$ any sequence of numerical digit characters.\n",
      "\n",
      "In many cases, the first non-whitespace character can be used to deduce \n",
      "the kind of token that follows and subsequent input characters are then \n",
      "processed one at a time until reaching a character that is not in the \n",
      "set of characters acceptable for that token (this is termed the maximal munch, \n",
      "or longest match, rule).\n",
      "In some languages, 1492 the lexeme creation rules \n",
      "are more complex and may involve backtracking over previously read characters.\n",
      "\n",
      "For example, in C, one 'L' character is not enough to distinguish between an identifier \n",
      "that begins with 'L' and a wide-character string literal.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "sent_token_model = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "file_doc = sent_token_model(file_content)\n",
    "\n",
    "for sentence in file_doc.sents:\n",
    "    print(sentence.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e2178b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Lexical', 'tokenization', 'is', 'conversion', 'of', 'a', 'text', 'into', '(', 'semantically', 'or', 'syntactically', ')', 'meaningful', '\\n', 'lexical', 'tokens', 'belonging', 'to', 'categories', 'defined', 'by', 'a', '\"', 'lexer', '\"', 'program', '.', '\\n', 'In', 'case', 'of', 'a', 'natural', 'language', ',', 'those', 'categories', 'include', 'nouns', ',', 'verbs', ',', '\\n', 'adjectives', ',', 'punctuations', 'etc', '.', 'In', 'case', 'of', 'a', 'programming', 'language', ',', '\\n', 'the', 'categories', 'include', 'identifiers', ',', 'operators', ',', 'grouping', 'symbols', 'and', 'data', 'types', '.', '\\n', 'Lexical', 'tokenization', 'is', 'not', 'the', 'same', 'process', 'as', 'the', 'probabilistic', 'tokenization', ',', '\\n', 'used', 'for', 'large', 'language', 'model', \"'s\", '3689', 'data', 'preprocessing', ',', 'that', 'encode', 'text', 'into', 'numerical', 'tokens', ',', 'using', 'byte', 'pair', 'encoding', '.', 'The', 'first', 'stage', ',', 'the', 'scanner', ',', '\\n', 'is', 'usually', 'based', 'on', 'a', 'finite', '-', 'state', 'machine', '(', 'FSM', ')', '.', 'It', 'has', 'encoded', 'within', 'it', 'information', 'on', '\\n', 'the', 'possible', 'sequences', 'of', 'characters', 'that', 'can', 'be', 'contained', 'within', 'any', 'of', 'the', 'tokens', 'it', 'handles', '\\n', '(', 'individual', 'instances', 'of', 'these', 'character', 'sequences', 'are', 'termed', 'lexemes', ')', '.', 'For', 'example', ',', '\\n', 'an', 'integer', 'lexeme', 'may', 'contain', '!', '@%$', 'any', 'sequence', 'of', 'numerical', 'digit', 'characters', '.', '\\n', 'In', 'many', 'cases', ',', 'the', 'first', 'non', '-', 'whitespace', 'character', 'can', 'be', 'used', 'to', 'deduce', '\\n', 'the', 'kind', 'of', 'token', 'that', 'follows', 'and', 'subsequent', 'input', 'characters', 'are', 'then', '\\n', 'processed', 'one', 'at', 'a', 'time', 'until', 'reaching', 'a', 'character', 'that', 'is', 'not', 'in', 'the', '\\n', 'set', 'of', 'characters', 'acceptable', 'for', 'that', 'token', '(', 'this', 'is', 'termed', 'the', 'maximal', 'munch', ',', '\\n', 'or', 'longest', 'match', ',', 'rule', ')', '.', 'In', 'some', 'languages', ',', '1492', 'the', 'lexeme', 'creation', 'rules', '\\n', 'are', 'more', 'complex', 'and', 'may', 'involve', 'backtracking', 'over', 'previously', 'read', 'characters', '.', '\\n', 'For', 'example', ',', 'in', 'C', ',', 'one', \"'\", 'L', \"'\", 'character', 'is', 'not', 'enough', 'to', 'distinguish', 'between', 'an', 'identifier', '\\n', 'that', 'begins', 'with', \"'\", 'L', \"'\", 'and', 'a', 'wide', '-', 'character', 'string', 'literal', '.']\n"
     ]
    }
   ],
   "source": [
    "print([words.text for words in file_doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5277179b",
   "metadata": {},
   "source": [
    "# \"\\n\" is replaced with \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7e1650cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Lexical tokenization is conversion of a text into (semantically or syntactically) meaningful lexical tokens belonging to categories defined by a \"lexer\" program.', 'In case of a natural language, those categories include nouns, verbs, adjectives, punctuations etc.', 'In case of a programming language, the categories include identifiers, operators, grouping symbols and data types.', \"Lexical tokenization is not the same process as the probabilistic tokenization, used for large language model's 3689 data preprocessing, that encode text into numerical tokens, using byte pair encoding.\", 'The first stage, the scanner, is usually based on a finite-state machine (FSM).', 'It has encoded within it information on the possible sequences of characters that can be contained within any of the tokens it handles (individual instances of these character sequences are termed lexemes).', 'For example, an integer lexeme may contain !', '@%$ any sequence of numerical digit characters.', 'In many cases, the first non-whitespace character can be used to deduce the kind of token that follows and subsequent input characters are then processed one at a time until reaching a character that is not in the set of characters acceptable for that token (this is termed the maximal munch, or longest match, rule).', 'In some languages, 1492 the lexeme creation rules are more complex and may involve backtracking over previously read characters.', \"For example, in C, one 'L' character is not enough to distinguish between an identifier that begins with 'L' and a wide-character string literal.\"]\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en import English\n",
    "\n",
    "model = English()\n",
    "\n",
    "model.add_pipe('sentencizer')\n",
    "\n",
    "sentence_list =[]\n",
    "for sentence in file_doc.sents:\n",
    "    sentence_list.append(sentence.text.replace(\"\\n\",\"\"))\n",
    "print(sentence_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb0ff8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
